<?xml version='1.0' encoding='utf8'?>
<article><preamble>Stolcke_1996_Automatic linguistic</preamble><titre>AUTOMATIC LINGUISTIC SEGMENTATION
OF CONVERSATIONAL SPEECH</titre><auteur>Elizabeth ShribergSpeech Technology and Research LaboratorySRI International, Menlo Park, CA 94025stolcke@speech.sri.com ees@speech.sri.comABSTRACTAs speech recognition moves toward more unconstrained domainssuch as conversational speech, we encounter a need to be able tosegment (or resegment) waveforms and recognizer output into linguistically meaningful units, such a sentences. Toward this end,we present a simple automatic segmenter of transcripts based onN-gram language modeling. We also study the relevance of several word-level features for segmentation performance. Using onlyword-level information, we achieve 85% recall and 70% precisionon linguistic boundary detection.1.INTRODUCTIONToday’s large-vocabulary speech recognizers typically prefer to process a few tens of seconds of speech at a time, to keep the time andmemory demands of the decoder within bounds. For longer inputs,the waveform is usually presegmented into shorter pieces based onsimple acoustic criteria, such as nonspeech intervals (e.g., pauses)and turn boundaries (when several speakers are involved). We referto such segmentations as acoustic segmentations.Acoustic segmentations generally do not reflect the linguistic structure of utterances. They may fragment sentences or semantic units,or group together spans of unrelated units. We examine several reasons why such behavior is undesirable, and propose that linguisticsegmentations be used instead. This requires algorithms for automatically finding linguistic units. In this paper we report on firstresults from our ongoing efforts toward such an automatic linguistic segmentation. In all further discussion, unless otherwise noted,the terms ‘segment,’ ‘segmentation,’ etc. will refer to linguistic segmentations.2. THE IMPORTANCE OF LINGUISTICSEGMENTATIONAcoustic segmentations are inadequate in cases where the outputof a speech recognizer is to serve as input for further processingbased on syntactically or semantically coherent units. This includesmost natural language (NL) parsers or NL understanding or translation systems. For such systems, the fragmented recognition outputwould have to be put back together and large spans of unrelatedmaterial would need to be resegmented into linguistic units.Automatic detection of linguistic segments could also improve theuser interface of many speech systems. A spoken language systemcould use the knowledge incorporated in an automatic segmenterto help end-point a user’s speech input. A speech indexing and retrieval system (such as for transcribed broadcast audio) could process its data in more meaningful units if the locations of linguisticsegment boundaries were known.Our main motivation for the work reported here comes from speechlanguage modeling. Experiments at the 1995 Johns Hopkins Language Modeling Workshop showed that the quality of a languagemodel (LM) can be improved if both training and test data are segmented linguistically, rather than acoustically [8]. We showed in[10] and [9] that proper modeling of filled pauses requires knowledge of linguistic segment boundaries. We found for example thatsegment-internal filled pauses condition the following words quitedifferently from segment-initial filled pauses. Finally, recent effortsin language modeling for conversational speech, such as [8], attemptto capitalize on the internal structure of utterances and turns. Suchmodels are formulated in terms of linguistic units and therefore require linguistic segmentations to be applicable.3.METHODOur main goal for this work was to examine to what extent variouskinds of lexical (word-based) information were useful for automaticlinguistic segmentation. This precluded a study based on the output of existing speech recognition systems, which currently achieveabout 40-50% word error rate on the type of data used in our experiments. At such high error rates, the analysis of any segmentationalgorithm and the features it uses would likely be confounded bythe unreliable nature of the input data. We therefore chose to eliminate the problem of inaccurate speech recognition and tested our algorithms on hand-transcribed word-level transcripts of spontaneousspeech from the Switchboard corpus [4]. An additional benefit ofthis approach is that the models employed by the segmentation algorithms can also be directly used as language models for speechrecognizers for the same type of data, an application we are pursuing as well.The segmentation approaches we investigated all fell within the following framework. We first trained a statistical language modelof the N-gram variety to model the distribution of both words andsegment boundaries. (For this purpose, segment boundaries wererepresented as special tokens &lt;s&gt; within the text.) The segmentation information was removed from the test data, and the languagemodel was used to hypothesize the most probable locations of seg-ment boundaries. The resulting segmentations were then evaluatedalong a number of metrics.computation yields the likelihoods of the states at each position k:As training data, we used 1.4 million words of Switchboard transcripts annotated for linguistic segmentations by the UPenn Treebank project [7], comprising a total of 193,000 segments. One halfof the standard Switchboard development test set, totaling 10,000words and 1,300 segments, was used for testing.PNO-S (w1 : : : wk )The hand-annotated segments encompassed different kinds of linguistic structures, includingPS (w1 : : : wk )1)p(wkjwkp(wkj&lt;s&gt;wk=PNO-S (w1 : : : wkp(&lt;s&gt;jwk1)1)j2 wk 1 ) p(wk &lt;s&gt;)+PS (w1 : : : wk 1 )p(&lt;s&gt;j&lt;s&gt;wkStand-alone phrasesj1 ) p(wk &lt;s&gt;)Disfluent sentences aborted in mid-utterance1B.44: Worried that they're not going toget enough attention? &lt;s&gt; //A corresponding Viterbi algorithm is used to find the most likelysequence of S and NO-S (i.e., a segmentation) for a given wordstring. This language model is a full implementation of the modelapproximated in [8]. The hidden disfluency model of [10] has asimilar structure. As indicated in the formulae above, we currentlyuse at most two words of history in the local conditional probabilities p(j). Longer N-grams can be used if more state information iskept.A.45: Yeah, &lt;s&gt; and, uh, you know, coldsand things like that &lt;laughter&gt; get -- //The local N-gram probabilities are estimated from the training databy using Katz backoff with Good-Turing discounting [6].Interjections and back-channel responsesB.46:Yeah.&lt;s&gt; //A.47: -- spread real easy and things,&lt;s&gt; but, // and they're expensive &lt;s&gt; and,// &lt;lipsmack&gt; // course, // there's a lotof different types of day care available,too, // you know, where they teach themacademic things. &lt;s&gt; //B.48:Yes.&lt;s&gt; //This short transcript shows some of the ubiquitous features of spontaneous speech affecting segmentation, such as1)2 wkPNO-S (w1 : : : wk+PS (w1 : : : wk 1 )Complete sentencesThe following excerpt illustrates the character of the data. Linguistic segment boundaries are marked &lt;s&gt;, whereas acoustic segmentations are indicated by //.=Mismatch between acoustic and linguistic segmentations(A.47)segments spanning several turns (A.45 and A.47)backchannel responses (B.46)5. RESULTS5.1.Baseline Segmentation ModelThe first model we looked at models only plain words and segmentboundaries in the manner described. It was applied to the concatenation of all turns of a conversation side, with no additional contextual cues supplied. During testing, this model thus operates withvery minimal information, i.e., with only the raw word sequence tobe segmented. Table 1 shows results for bigram and trigram models. The performance metrics used are defined as follows. RecallTable 1: Baseline model performanceModelRecall PrecisionFASERBigram65.5%56.9%1.9% 58.9%Trigram 70.2%60.7%2.0% 53.1%4. THE MODELThe language models used were of the N-gram type commonly usedin speech recognition [5]. In N-gram models, a word wn from an 1 word history w1 : : : wn 1. If the history contains a segmentboundary &lt;s&gt;, it is truncated before that location. During testing,the model is run as a hidden segment model, hypothesizing segmentboundaries between any two words and implicitly computing theprobabilities of all possible segmentations.Associated with each word position are two states, S and NO-S, corresponding to a segment starting or not before that word. A forward1 Although complete and disfluent sentences were marked differently inthe corpus, we modeled these with a single type of boundary token.is the percentage of actual segment boundaries hypothesized. Precision is the percentage of hypothesized boundaries that are actual.False Alarms (FA) are the fraction of potential boundaries incorrectly hypothesized as boundaries. Segment Error Rate (SER) is thepercentage of actual segments identified without intervening falsealarms.As can be seen, word context alone can identify a majority of segment boundaries at a modest false alarm rate of about 2%. The trigram model does better than the bigram, but this is expected since ithas access to a larger context around potential segment boundaries.to use in its decision. Given these results, we only consider trigrammodels in all following experiments.5.2.Using Turn InformationNext we examined a richer model that incorporated informationabout the turn-taking between speakers.2 Note that turn boundariesare already present in acoustic segmentations, but in this case wewill only use them as a cue to the identification of linguistic segments. Turn information is easily incorporated into the segmentation model by placing special tags at turn boundaries (in both training and testing). Model performance is summarized in Table 2.Table 2: Segmentation performance using turn informationModelRecall PrecisionFASERBaseline70.2%60.7%2.0% 53.1%Turn-tagged 76.9%66.9%1.8% 44.9%As can be seen, adding turn information improves performance onall metrics. This improvement occurs even though turn boundariesare far from perfectly correlated with segment boundaries. As illustrated earlier, turns can contain multiple segments, or segments mayspan multiple turns.5.3.boundaries provide some of the strongest cues for these boundaries.Apart from these strong lexical cues, it seems to be helpful to abstract from word identity and use POS information instead. In otherwords, the tag set could be optimized to provide the right level ofresolution for the segmentation task.It should be noted that the results for POS-based models are optimistic in the sense that for an actual application one would firsthave to tag the input with POS labels, and then apply the segmentation model. The actual performance would be degraded by taggingerrors.5.4.Error Trade-offsAs an aside to our search for useful features for the segmentation task, we observe that we can optimize any particular languagemodel by trading off recall performance for false alarm rate, or viceversa. We did this by biasing the likelihoods of S states by someconstant factor, causing the Viterbi algorithm to choose these statesmore often. Table 4 compares two bias values, and shows that thebias can be used to increase both recall and precision, while alsoreducing the segment error rate.Using Part-of-Speech InformationSo far we have used only the identity of words. It is likely thatsegmentation is closely related to syntactic (as opposed to lexical)structure. Short of using a full-scale parser on the input we coulduse the parts of speech (POS) of words as a more suitable representation from which to predict segment boundaries. Parts of speechshould also generalize much better to contexts containing N-gramsnot observed in the training data (assuming the POS of the wordsinvolved is known).We were able to test this hypothesis by using the POS-tagged version of the Switchboard corpus. We built two models based on POSfrom this data. Model I had all words replaced by their POS labelsduring training and test, and also used turn boundary information.Model II also used POS labels, but retained the word identities ofcertain word classes that were deemed to be particularly relevant tosegmentation. These retained words include filled pauses, conjunctions, and certain discourse markers such as “okay,” “so,” “well,”etc. Results are shown in Table 3.Table 3: Segmentation performance using POS informationModelRecall PrecisionFASERWord-based76.9%66.9%1.8% 44.9%POS-based I68.9%58.5%2.0% 59.3%POS-based II 79.6%73.5%0.9% 39.9%We see that POS tags alone (Model I) do not result in better segmentations than words. The fact that Model II performs better than boththe all-word based model and the pure POS model indicates thatcertain function words that tend to occur in the context of segment2 Speakers can talk over each other. We did not model this case separately; instead, we adopted the serialization of turns implied by the transcripts.ModelBias = 1Bias = 2Table 4: Biasing segmentationRecall PrecisionFA76.9%66.9%1.8%85.2%69.2%2.7%SER44.9%37.4%6. DISCUSSION6.1.Error AnalysisTo understand what type of errors the segmenter makes, we handchecked a set of 200 false alarms generated by the baseline trigrammodel. The most frequent type (34%) of false alarm correspondedto splitting of segments at sentence-internal clause boundaries, e.g.,false alarms triggered by a conjunction that would be likely to starta segment. For example, the &lt;s&gt; in the segmentationi'm not sure how many active volcanosthere are now &lt;s&gt; and and what the amountof material that they do uh put into theatmosphererepresents a false alarm, presumably triggered by the following coordinating conjunction “and.”5% of the false alarms could be attributed to filled pauses at theend of segments, which were often attached to the following segment. This actually reflects a labeling ambiguity that should not becounted as an error. Another 7% of the false alarm we deemed tobe labeling errors. Thus, a total of 12% of false alarms could beconsidered to be actually correct.6.2.Other Segmentation AlgorithmsOur language-model-based segmentation algorithm is only one ofmany that could be used to perform the linguistic segmentation task,given a set of features. Conceptually, segmentation is just anotherclassification problem, in which each word transition must be labeled as either a segment boundary or a within-segment transition.Two natural choices for alternative approaches are decision treesand a transformation-based, error-driven classifier of the type developed by Eric Brill for other tagging problems [2]. Both of thesemethods would make it easier to combine diverse input features thatare not readily integrated into a single probabilistic language model,e.g., if we wanted to use both POS and word identity for each word.3Our approach, on the other hand, has the advantage of simplicityand efficiency. Furthermore, the language model used for segmentation can also be used for speech decoding or rescoring.We already mentioned that if POS information is to be used forsegmentation, an automatic tagging step is required. This presentssomewhat of a chicken-and-egg problem, in that taggers typicallyrely on segmentations. An appealing solution to this problem in thestatistical tagging framework [3] would be to model both segmentation and tag assignment as a single hidden Markov process.6.3.Other Features for SegmentationAll of our experiments were based on lexical information only. Tofurther improve segmentation performance, and to make it less dependent on accurate speech recognition, we plan to combine the LMapproach with a model for various acoustic and prosodic correlatesof segmentation. These include:Unfilled pause durationsFundamental frequency patternsPhone durationsGlottalizationOur current segmentation model deals with each conversation sidein isolation. An alternative approach is to model the two sidesjointly, thereby allowing us to capitalize on correlations between thesegment structure of one speaker and what is said by the other. It islikely, for example, that backchannel responses would be modeledbetter this way.7. CONCLUSIONSWe have argued for the need for automatic speech segmentation algorithms that can identify linguistically motivated, sentence-levelunits of speech. We have shown that transcribed speech can besegmented linguistically with good accuracy by using an N-gramlanguage model for the locations of the hidden segment boundaries.We studied several word-level features for possible incorporationin the model, and found that best performance so far was achievedwith a combination of function ‘cue’ words, POS labels, and turnmarkers.AcknowledgmentsThis research was supported by DARPA and NSF, under NSF GrantIRI-9314967. The views herein are those of the authors and shouldnot be interpreted as representing the policies of DARPA or theNSF.3 Such an integration can be achieved in a language model using the maximum entropy paradigm [1], but this would make the estimation processconsiderably more expensive.8.REFERENCES1. A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–71, 1996.2. E. Brill. Some advances in transformation-based part of speechtagging. In Proceedings of the 12th National Conference onArtificial Intelligence, Seattle, WA, 1994. AAAI Press.3. K. W. Church. A stochastic parts program and noun phraseparser for unrestricted text. In Second Conference on AppliedNatural Language Processing, pages 136–143, Austin, Texas,1988.4. J. J. Godfrey, E. C. Holliman, and J. McDaniel. SWITCHBOARD: Telephone speech corpus for research and development. In Proceedings IEEE Conference on Acoustics, Speechand Signal Processing, volume I, pages 517–520, San Francisco, March 1992.5. F. Jelinek. Self-organized language modeling for speech recognition. In A. Waibel and K.-F. Lee, editors, Readings in SpeechRecognition. Morgan Kaufmann, San Mateo, Ca., 1990.6. S. M. Katz. Estimation of probabilities from sparse data forthe language model component of a speech recognizer. IEEETransactions on Acoustics, Speech, and Signal Processing,35(3):400–401, March 1987.7. M. Meteer et al. Dysfluency annotation stylebook for theSwitchboard corpus. Distributed by LDC, February 1995. Revised June 1995 by Ann Taylor.8. M. Meteer and R. Iyer. Modeling conversational speech forspeech recognition. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, Philadelphia,PA, May 1996.9. E. Shriberg and A. Stolcke. Word predictability after hesitations: A corpus-based study. In Proceedings InternationalConference on Spoken Language Processing, Philadelphia, PA,October 1996.10. A. Stolcke and E. Shriberg. Statistical language modelingfor speech disfluencies. In Proceedings IEEE Conference onAcoustics, Speech and Signal Processing, volume I, pages 405–408, Atlanta, GA, May 1996.</auteur><abstract /><biblio>REFERENCES1. A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–71, 1996.2. E. Brill. Some advances in transformation-based part of speechtagging. In Proceedings of the 12th National Conference onArtificial Intelligence, Seattle, WA, 1994. AAAI Press.3. K. W. Church. A stochastic parts program and noun phraseparser for unrestricted text. In Second Conference on AppliedNatural Language Processing, pages 136–143, Austin, Texas,1988.4. J. J. Godfrey, E. C. Holliman, and J. McDaniel. SWITCHBOARD: Telephone speech corpus for research and development. In Proceedings IEEE Conference on Acoustics, Speechand Signal Processing, volume I, pages 517–520, San Francisco, March 1992.5. F. Jelinek. Self-organized language modeling for speech recognition. In A. Waibel and K.-F. Lee, editors, Readings in SpeechRecognition. Morgan Kaufmann, San Mateo, Ca., 1990.6. S. M. Katz. Estimation of probabilities from sparse data forthe language model component of a speech recognizer. IEEETransactions on Acoustics, Speech, and Signal Processing,35(3):400–401, March 1987.7. M. Meteer et al. Dysfluency annotation stylebook for theSwitchboard corpus. Distributed by LDC, February 1995. Revised June 1995 by Ann Taylor.8. M. Meteer and R. Iyer. Modeling conversational speech forspeech recognition. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, Philadelphia,PA, May 1996.9. E. Shriberg and A. Stolcke. Word predictability after hesitations: A corpus-based study. In Proceedings InternationalConference on Spoken Language Processing, Philadelphia, PA,October 1996.10. A. Stolcke and E. Shriberg. Statistical language modelingfor speech disfluencies. In Proceedings IEEE Conference onAcoustics, Speech and Signal Processing, volume I, pages 405–408, Atlanta, GA, May 1996.</biblio></article>